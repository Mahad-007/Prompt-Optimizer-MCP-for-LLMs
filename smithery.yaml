name: prompt-optimizer-mcp
description: A Model Context Protocol server for optimizing and scoring LLM prompts
version: 1.0.0

# Runtime configuration
runtime: python3.11

# Entry point
entrypoint: start.py

# Dependencies
dependencies:
  - fastmcp>=0.1.0
  - fastapi>=0.104.0
  - uvicorn>=0.24.0
  - pydantic>=2.0.0

# Environment variables
environment:
  PYTHONPATH: .
  PYTHONUNBUFFERED: "1"
  PYTHONDONTWRITEBYTECODE: "1"
  DEPLOYMENT_MODE: "http"
  PORT: "8000"
  HOST: "0.0.0.0"

# Build configuration
build:
  context: .
  dockerfile: Dockerfile

# Resources
resources:
  cpu: 0.5
  memory: 512Mi

# Scaling
scaling:
  min_replicas: 1
  max_replicas: 3
  target_cpu_utilization: 70

# Health check
health_check:
  path: /
  port: 8000
  initial_delay_seconds: 10
  period_seconds: 30
  timeout_seconds: 10
  failure_threshold: 3 